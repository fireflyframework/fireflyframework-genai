# Copyright 2026 Firefly Software Solutions Inc
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Code generator: converts a visual graph model into executable Python code.

The generated code uses the fireflyframework-genai public API so that users
can "eject" their visual pipeline into a standalone Python script.  All node
types are supported: agent, tool, reasoning, condition, memory, validator,
custom_code, fan_out, fan_in, input, and output.
"""

from __future__ import annotations

from pathlib import Path
from typing import Any

from fireflyframework_genai.studio.codegen.models import (
    GraphModel,
    GraphNode,
    NodeType,
)

# ---------------------------------------------------------------------------
# Constants
# ---------------------------------------------------------------------------

_MODULE_DOCSTRING = '"""Pipeline generated by Firefly Agentic Studio."""'
_FUTURE_IMPORT = "from __future__ import annotations"

_FALLBACK_MODEL = "openai:gpt-4o"


def _get_default_model(settings_path: Path | None = None) -> str:
    """Read the user's configured default model from settings."""
    from fireflyframework_genai.studio.settings import load_settings

    settings = load_settings(path=settings_path)
    return settings.model_defaults.default_model or _FALLBACK_MODEL


# ---------------------------------------------------------------------------
# Public API
# ---------------------------------------------------------------------------


def generate_python(graph: GraphModel, settings_path: Path | None = None) -> str:
    """Convert a :class:`GraphModel` into valid, human-readable Python code.

    Supports all node types: agent, tool, reasoning, condition, input, output,
    memory, validator, custom_code, fan_out, and fan_in.  The generated code
    mirrors the compiler logic so that ejected pipelines behave identically
    to their visual canvas equivalents.
    """
    default_model = _get_default_model(settings_path)

    # Collect node types present to determine required imports
    type_set = {n.type for n in graph.nodes}
    has_edges = len(graph.edges) > 0

    lines: list[str] = []

    # -- Module docstring & future import ------------------------------------
    lines.append(_MODULE_DOCSTRING)
    lines.append(_FUTURE_IMPORT)
    lines.append("")

    # -- Imports -------------------------------------------------------------
    lines.extend(_emit_imports(type_set, has_edges))
    lines.append("")
    lines.append("")

    # -- Node definitions (one block per node) -------------------------------
    for node in graph.nodes:
        block = _emit_node(node, default_model)
        if block:
            lines.append(block)
            lines.append("")

    # -- Pipeline builder (only when edges exist) ----------------------------
    if has_edges:
        lines.append("")
        lines.append(_emit_pipeline(graph))
        lines.append("")
        lines.append("")
        lines.append(_emit_main_block(graph))

    return "\n".join(lines)


# ---------------------------------------------------------------------------
# Import emission
# ---------------------------------------------------------------------------


def _emit_imports(type_set: set[NodeType], has_edges: bool) -> list[str]:
    """Emit only the imports needed for the node types present."""
    imports: list[str] = []

    if NodeType.AGENT in type_set:
        imports.append("from fireflyframework_genai.agents.base import FireflyAgent")

    if NodeType.TOOL in type_set:
        imports.append("from fireflyframework_genai.tools.registry import tool_registry")

    if NodeType.REASONING in type_set:
        imports.append("from fireflyframework_genai.agents.registry import agent_registry")
        imports.append("from fireflyframework_genai.reasoning.registry import reasoning_registry")

    if has_edges:
        imports.append("from fireflyframework_genai.pipeline.builder import PipelineBuilder")

        # Collect step types needed
        step_types: list[str] = []
        if NodeType.AGENT in type_set:
            step_types.append("AgentStep")
        if NodeType.REASONING in type_set:
            step_types.append("ReasoningStep")
        if NodeType.FAN_OUT in type_set:
            step_types.append("FanOutStep")
        if NodeType.FAN_IN in type_set:
            step_types.append("FanInStep")
        if NodeType.CONDITION in type_set:
            step_types.append("BranchStep")
        if any(
            t in type_set
            for t in (
                NodeType.TOOL,
                NodeType.MEMORY,
                NodeType.VALIDATOR,
                NodeType.CUSTOM_CODE,
                NodeType.INPUT,
                NodeType.OUTPUT,
                NodeType.PIPELINE_STEP,
            )
        ):
            step_types.append("CallableStep")

        if step_types:
            imports.append(
                f"from fireflyframework_genai.pipeline.steps import {', '.join(sorted(step_types))}"
            )

        imports.append("from fireflyframework_genai.pipeline.context import PipelineContext")

    if NodeType.MEMORY in type_set:
        imports.append("from fireflyframework_genai.memory.manager import MemoryManager")
        imports.append("from fireflyframework_genai.memory.store import FileStore")

    # asyncio for the main block
    if has_edges:
        imports.append("import asyncio")

    return imports


# ---------------------------------------------------------------------------
# Node emitters â€” one function per NodeType
# ---------------------------------------------------------------------------


def _emit_node(node: GraphNode, default_model: str) -> str:
    """Dispatch to the correct emitter based on node type."""
    emitter = _NODE_EMITTERS.get(node.type)
    if emitter is None:
        return f"# Unsupported node type: {node.type!r} (node {node.id!r})"
    return emitter(node, default_model)


def _emit_agent_node(node: GraphNode, default_model: str) -> str:
    """Emit a FireflyAgent definition."""
    name = _safe_var(node.id)
    model = node.data.get("model", default_model) or default_model
    instructions = node.data.get("instructions", "")
    description = node.data.get("description", "")

    parts = [f'{name} = FireflyAgent(']
    parts.append(f'    name="{node.id}",')
    parts.append(f'    model="{model}",')
    parts.append(f'    instructions={_format_string_literal(instructions)},')
    if description:
        parts.append(f'    description={_format_string_literal(description)},')
    parts.append(")")
    return "\n".join(parts)


def _emit_tool_node(node: GraphNode, _default_model: str) -> str:
    """Emit a tool step using the tool registry."""
    name = _safe_var(node.id)
    tool_name = node.data.get("tool_name", "")
    if not tool_name:
        return f"# TOOL node {node.id!r} is missing 'tool_name' configuration"

    lines = [
        f'# Tool node: {node.label or node.id}',
        f'{name}_tool = tool_registry.get("{tool_name}")',
        '',
        '',
        f'async def {name}_execute(context: PipelineContext, inputs: dict) -> dict:',
        f'    """Execute tool: {tool_name}."""',
        f'    return await {name}_tool.execute(**inputs)',
    ]
    return "\n".join(lines)


def _emit_reasoning_node(node: GraphNode, _default_model: str) -> str:
    """Emit a reasoning step."""
    name = _safe_var(node.id)
    pattern_name = node.data.get("pattern_name") or node.data.get("pattern", "")
    agent_name = node.data.get("agent_name", "")

    if not pattern_name:
        return f"# REASONING node {node.id!r} is missing 'pattern_name' configuration"

    lines = [
        f'# Reasoning node: {node.label or node.id}',
        f'{name}_pattern = reasoning_registry.get("{pattern_name}")',
    ]
    if agent_name:
        lines.append(f'{name}_agent = agent_registry.get("{agent_name}")')
    else:
        lines.append(f'# Note: REASONING node {node.id!r} has no agent_name; '
                      f'you may need to assign an agent here')
        lines.append(f'{name}_agent = None  # TODO: assign an agent')

    return "\n".join(lines)


def _emit_condition_node(node: GraphNode, _default_model: str) -> str:
    """Emit a condition/branch router."""
    name = _safe_var(node.id)
    condition_key = node.data.get("condition", "input")
    branches = node.data.get("branches", {})

    if not branches:
        return f"# CONDITION node {node.id!r} is missing 'branches' configuration"

    branches_repr = repr(branches)

    lines = [
        f'# Condition node: {node.label or node.id}',
        f'{name}_branches = {branches_repr}',
        f'{name}_default = next(iter({name}_branches.values()))',
        '',
        '',
        f'def {name}_router(inputs: dict) -> str:',
        f'    """Route based on key {condition_key!r}."""',
        f'    value = str(inputs.get("{condition_key}", ""))',
        f'    return {name}_branches.get(value, {name}_default)',
    ]
    return "\n".join(lines)


def _emit_fan_out_node(node: GraphNode, _default_model: str) -> str:
    """Emit a fan-out split function."""
    name = _safe_var(node.id)
    field = node.data.get("split_expression", "")

    lines = [
        f'# Fan-Out node: {node.label or node.id}',
        '',
        '',
        f'def {name}_split(value):',
        '    """Split input for parallel processing."""',
    ]
    if field:
        lines.extend([
            '    if isinstance(value, dict):',
            f'        extracted = value.get("{field}", value)',
            '        return list(extracted) if isinstance(extracted, list) else [extracted]',
            '    return list(value) if isinstance(value, list) else [value]',
        ])
    else:
        lines.extend([
            '    return list(value) if isinstance(value, list) else [value]',
        ])
    return "\n".join(lines)


def _emit_fan_in_node(node: GraphNode, _default_model: str) -> str:
    """Emit a fan-in merge function."""
    name = _safe_var(node.id)
    merge_expr = node.data.get("merge_expression", "collect")

    lines = [
        f'# Fan-In node: {node.label or node.id}',
        '',
        '',
        f'def {name}_merge(items: list):',
        f'    """Merge parallel outputs ({merge_expr})."""',
    ]
    if merge_expr == "concat":
        lines.extend([
            '    result = []',
            '    for item in items:',
            '        if isinstance(item, list):',
            '            result.extend(item)',
            '        else:',
            '            result.append(item)',
            '    return result',
        ])
    else:
        lines.append('    return items')
    return "\n".join(lines)


def _emit_memory_node(node: GraphNode, _default_model: str) -> str:
    """Emit a memory operation step."""
    name = _safe_var(node.id)
    action = node.data.get("memory_action", "retrieve")

    lines = [
        f'# Memory node: {node.label or node.id} (action: {action})',
        '',
        '',
        f'async def {name}_execute(context: PipelineContext, inputs: dict):',
        f'    """Memory operation: {action}."""',
        '    memory = context.memory',
        '    if memory is None:',
        '        return inputs.get("input")',
        '    key = inputs.get("key", "default")',
    ]

    if action == "store":
        lines.extend([
            '    value = inputs.get("input", inputs.get("value"))',
            '    memory.set_fact(key, value)',
            '    return value',
        ])
    elif action == "clear":
        lines.extend([
            '    memory.working.delete(key)',
            '    return None',
        ])
    else:  # retrieve
        lines.extend([
            '    return memory.get_fact(key)',
        ])
    return "\n".join(lines)


def _emit_validator_node(node: GraphNode, _default_model: str) -> str:
    """Emit a validator step."""
    name = _safe_var(node.id)
    rule = node.data.get("validation_rule", "not_empty")

    lines = [
        f'# Validator node: {node.label or node.id} (rule: {rule})',
        '',
        '',
        f'async def {name}_validate(context: PipelineContext, inputs: dict):',
        f'    """Validate input: {rule}."""',
        '    value = inputs.get("input", context.inputs)',
    ]

    if rule == "not_empty":
        lines.extend([
            '    if not value:',
            '        raise ValueError("Validation failed: value is empty")',
        ])
    elif rule == "is_string":
        lines.extend([
            '    if not isinstance(value, str):',
            '        raise TypeError(f"Expected string, got {type(value).__name__}")',
        ])
    elif rule == "is_list":
        lines.extend([
            '    if not isinstance(value, list):',
            '        raise TypeError(f"Expected list, got {type(value).__name__}")',
        ])
    elif rule == "is_dict":
        lines.extend([
            '    if not isinstance(value, dict):',
            '        raise TypeError(f"Expected dict, got {type(value).__name__}")',
        ])
    elif rule:
        lines.extend([
            f'    if isinstance(value, dict) and "{rule}" not in value:',
            f'        raise KeyError("Missing required key: {rule}")',
        ])

    lines.append('    return value')
    return "\n".join(lines)


def _emit_custom_code_node(node: GraphNode, _default_model: str) -> str:
    """Emit a custom code node (inline user code)."""
    name = _safe_var(node.id)
    code = node.data.get("code", "")

    if not code:
        return f"# CUSTOM_CODE node {node.id!r} has no code defined"

    lines = [
        f'# Custom Code node: {node.label or node.id}',
        '# The code below must define: async def execute(context, inputs) -> Any',
    ]
    # Indent user code inside a namespace to avoid collisions
    for code_line in code.splitlines():
        lines.append(code_line)

    lines.extend([
        '',
        f'{name}_execute = execute  # bind to node variable',
    ])
    return "\n".join(lines)


def _emit_input_node(node: GraphNode, _default_model: str) -> str:
    """Emit an Input boundary node."""
    name = _safe_var(node.id)
    trigger_type = node.data.get("trigger_type", "manual")

    lines = [
        f'# Input node: {node.label or node.id} (trigger: {trigger_type})',
        '',
        '',
        f'async def {name}_step(context: PipelineContext, inputs: dict):',
        f'    """Pipeline entry point ({trigger_type} trigger)."""',
        '    return inputs.get("input", context.inputs)',
    ]

    # Add config comments for non-manual triggers
    if trigger_type == "http":
        http = node.data.get("http_config", {})
        if http:
            method = http.get("method", "POST")
            path = http.get("path_suffix", "")
            lines.insert(1, f'# HTTP config: {method} {path}')
    elif trigger_type == "queue":
        q = node.data.get("queue_config", {})
        if q:
            broker = q.get("broker", "")
            topic = q.get("topic_or_queue", "")
            lines.insert(1, f'# Queue config: {broker} / {topic}')
    elif trigger_type == "schedule":
        sched = node.data.get("schedule_config", {})
        if sched:
            cron = sched.get("cron_expression", "")
            lines.insert(1, f'# Schedule config: {cron}')
    elif trigger_type == "file_upload":
        fc = node.data.get("file_config", {})
        if fc:
            types = fc.get("accepted_types", ["*/*"])
            lines.insert(1, f'# File upload config: accepts {types}')

    return "\n".join(lines)


def _emit_output_node(node: GraphNode, _default_model: str) -> str:
    """Emit an Output boundary node."""
    name = _safe_var(node.id)
    dest_type = node.data.get("destination_type", "response")

    lines = [
        f'# Output node: {node.label or node.id} (destination: {dest_type})',
        '',
        '',
        f'async def {name}_step(context: PipelineContext, inputs: dict):',
        f'    """Pipeline exit point ({dest_type} destination)."""',
        f'    context.metadata["_output_config"] = {repr(node.data)}',
        '    return inputs.get("input", inputs)',
    ]

    if dest_type == "webhook":
        wh = node.data.get("webhook_config", {})
        if wh:
            url = wh.get("url", "")
            lines.insert(1, f'# Webhook config: {url}')
    elif dest_type == "store":
        sc = node.data.get("store_config", {})
        if sc:
            storage = sc.get("storage_type", "file")
            path = sc.get("path_or_table", "")
            lines.insert(1, f'# Store config: {storage} / {path}')

    return "\n".join(lines)


def _emit_pipeline_step_node(node: GraphNode, _default_model: str) -> str:
    """Emit a generic pipeline step (pass-through)."""
    name = _safe_var(node.id)
    return (
        f'# Pipeline step: {node.label or node.id}\n'
        f'\n\n'
        f'async def {name}_step(context: PipelineContext, inputs: dict):\n'
        f'    """Pass-through step."""\n'
        f'    return inputs.get("input", context.inputs)'
    )


_NODE_EMITTERS: dict[NodeType, Any] = {
    NodeType.AGENT: _emit_agent_node,
    NodeType.TOOL: _emit_tool_node,
    NodeType.REASONING: _emit_reasoning_node,
    NodeType.CONDITION: _emit_condition_node,
    NodeType.FAN_OUT: _emit_fan_out_node,
    NodeType.FAN_IN: _emit_fan_in_node,
    NodeType.MEMORY: _emit_memory_node,
    NodeType.VALIDATOR: _emit_validator_node,
    NodeType.CUSTOM_CODE: _emit_custom_code_node,
    NodeType.INPUT: _emit_input_node,
    NodeType.OUTPUT: _emit_output_node,
    NodeType.PIPELINE_STEP: _emit_pipeline_step_node,
}


# ---------------------------------------------------------------------------
# Pipeline builder emission
# ---------------------------------------------------------------------------


def _emit_pipeline(graph: GraphModel) -> str:
    """Emit a PipelineBuilder block that wires all nodes and edges."""
    parts: list[str] = []
    parts.append("# Build the pipeline")
    parts.append("pipeline = (")
    parts.append('    PipelineBuilder("pipeline")')

    for node in graph.nodes:
        step_expr = _step_expression(node)
        parts.append(f'    .add_node("{node.id}", {step_expr})')

    for edge in graph.edges:
        source_handle = edge.source_handle if edge.source_handle != "output" else ""
        target_handle = edge.target_handle if edge.target_handle != "input" else ""
        extra_args = ""
        if source_handle:
            extra_args += f', output_key="{edge.source_handle}"'
        if target_handle:
            extra_args += f', input_key="{edge.target_handle}"'
        parts.append(f'    .add_edge("{edge.source}", "{edge.target}"{extra_args})')

    parts.append("    .build()")
    parts.append(")")
    return "\n".join(parts)


def _step_expression(node: GraphNode) -> str:
    """Return the Python expression for a step in the pipeline builder."""
    name = _safe_var(node.id)

    if node.type == NodeType.AGENT:
        return f"AgentStep({name})"
    elif node.type == NodeType.TOOL:
        return f"CallableStep({name}_execute)"
    elif node.type == NodeType.REASONING:
        return f"ReasoningStep({name}_pattern, {name}_agent)"
    elif node.type == NodeType.CONDITION:
        return f"BranchStep({name}_router)"
    elif node.type == NodeType.FAN_OUT:
        return f"FanOutStep({name}_split)"
    elif node.type == NodeType.FAN_IN:
        merge_expr = node.data.get("merge_expression", "collect")
        if merge_expr == "collect":
            return "FanInStep()"
        return f"FanInStep({name}_merge)"
    elif node.type == NodeType.MEMORY:
        return f"CallableStep({name}_execute)"
    elif node.type == NodeType.VALIDATOR:
        return f"CallableStep({name}_validate)"
    elif node.type == NodeType.CUSTOM_CODE:
        return f"CallableStep({name}_execute)"
    elif node.type == NodeType.INPUT:
        return f"CallableStep({name}_step)"
    elif node.type == NodeType.OUTPUT:
        return f"CallableStep({name}_step)"
    elif node.type == NodeType.PIPELINE_STEP:
        return f"CallableStep({name}_step)"
    else:
        return f"# Unknown node type: {node.type}"


def _emit_main_block(graph: GraphModel) -> str:
    """Emit an ``if __name__`` block to run the pipeline."""
    # Check if there's an input node to determine what to pass
    input_nodes = [n for n in graph.nodes if n.type == NodeType.INPUT]

    lines = [
        'if __name__ == "__main__":',
        '',
        '    async def main():',
        '        from fireflyframework_genai.pipeline.context import PipelineContext',
    ]

    has_memory = any(n.type == NodeType.MEMORY for n in graph.nodes)
    if has_memory:
        lines.append('        memory = MemoryManager(store=FileStore(base_dir="./memory"))')
        lines.append('        context = PipelineContext(memory=memory)')
    else:
        lines.append('        context = PipelineContext()')

    if input_nodes:
        trigger = input_nodes[0].data.get("trigger_type", "manual")
        lines.append(f'        # Trigger type: {trigger}')

    lines.extend([
        '        result = await pipeline.run(context, inputs={"input": "Hello, pipeline!"})',
        '        print("Pipeline result:", result)',
        '',
        '    asyncio.run(main())',
    ])
    return "\n".join(lines)


# ---------------------------------------------------------------------------
# Utilities
# ---------------------------------------------------------------------------


def _safe_var(node_id: str) -> str:
    """Convert a node ID into a valid Python identifier."""
    return node_id.replace("-", "_").replace(" ", "_").replace(".", "_")


def _format_string_literal(value: str) -> str:
    """Return a Python string literal for *value*.

    * Simple single-line strings without problematic characters use ``"..."``.
    * Multi-line strings (containing ``\\n``) use triple-quoted strings so the
      generated code is readable.
    * Strings containing triple-double-quotes fall back to triple-single-quotes
      (and vice-versa) to guarantee valid syntax.
    * Backslashes are always properly escaped.
    """
    if not value:
        return '""'

    has_newline = "\n" in value

    if not has_newline:
        return _repr_double_quoted(value)

    escaped = value.replace("\\", "\\\\")

    has_triple_double = '"""' in escaped
    has_triple_single = "'''" in escaped

    if not has_triple_double:
        inner = escaped.replace('"', '\\"') if escaped.endswith('"') else escaped
        if '"""' not in inner:
            return f'"""{inner}"""'

    if not has_triple_single:
        inner = escaped.replace("'", "\\'") if escaped.endswith("'") else escaped
        if "'''" not in inner:
            return f"'''{inner}'''"

    inner = escaped.replace('"""', '\\"\\"\\"')
    return f'"""{inner}"""'


def _repr_double_quoted(value: str) -> str:
    """Return a double-quoted Python string literal for *value*."""
    r = repr(value)

    if r.startswith("'") and r.endswith("'"):
        inner = r[1:-1]
        inner = inner.replace("\\'", "'")
        inner = inner.replace('"', '\\"')
        return f'"{inner}"'

    return r
